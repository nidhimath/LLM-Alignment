{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nUNeQXMOiqz-"},"outputs":[],"source":["!pip install -q transformers torch accelerate langchain langchain-community langchain_huggingface"]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","import torch\n","import os\n","from google.colab import userdata\n","os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n","\n","# https://www.arxiv.org/pdf/2511.07772: emergent :("],"metadata":{"id":"Y_isM6vojt5x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", dtype=torch.bfloat16, device_map=\"auto\")\n","\n","# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n","messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"You always respond in the style of a teacher.\",\n","    },\n","    {\"role\": \"user\", \"content\": \"How does addition work?\"},\n","]\n","prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.8, top_k=50, top_p=0.95)\n","print(outputs[0][\"generated_text\"])"],"metadata":{"id":"h0Hc1v3WjCaH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Old Models"],"metadata":{"id":"ZgrlRMIF6zM7"}},{"cell_type":"code","source":["### Gemma is gated model, you need hugging face access: https://huggingface.co/google/gemma-2b\n","### To get your HF_TOKEN: https://huggingface.co/settings/tokens\n","\n","import os\n","from google.colab import userdata\n","\n","# Select the key icon on the right, add (HF_TOKEN, your_token) to secrets and allow notebook access\n","# Colab should save this secret across sessions\n","os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n","\n","MODEL = \"google/gemma-2-2b\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","model = AutoModelForCausalLM.from_pretrained(MODEL)\n","\n","prompt = \"Are dogs better than cats?\"\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","output = model.generate(**inputs, max_new_tokens=50)\n","print(tokenizer.decode(output[0], skip_special_tokens=True))"],"metadata":{"id":"O0KhB8lBjHPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL = \"HuggingFaceTB/smollm3-3b\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL)\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL,\n","    device_map=\"auto\",\n","    dtype=torch.float16\n",")\n","\n","prompt = \"Describe SmolLM briefly.\"\n","inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","output = model.generate(**inputs, max_new_tokens=50)\n","print(tokenizer.decode(output[0], skip_special_tokens=True))"],"metadata":{"id":"G1cvRyEyjKx2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Simple Constitutional AI with Langchain\n","This free method is purely prompt-based at inference time, no changes to model weights. It should be model agnostic and allows for a user-defined constitution. Unfortunately, model requires GPU to run fast."],"metadata":{"id":"MV4W96YB0jrk"}},{"cell_type":"code","source":["from transformers import pipeline\n","from langchain.llms import HuggingFacePipeline\n","from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple\n","from langchain.prompts import PromptTemplate\n","from langchain.chains import LLMChain, ConstitutionalChain\n","\n","MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n","pipe = pipeline(\"text-generation\", model=MODEL, dtype=torch.bfloat16, device_map=\"auto\")\n","\n","print(f\"Model name: {MODEL}\")\n","llm = HuggingFacePipeline(pipeline=pipe)\n","\n","principles = [\n","    ConstitutionalPrinciple(\n","        name=\"Harmlessness\",\n","        critique_request=\"Explain whether the response includes harmful or dangerous advice.\",\n","        revision_request=\"Rewrite the response to avoid harmful or dangerous content.\"\n","    )\n","]\n","\n","prompt = PromptTemplate(\n","    template=\"Answer the following question:\\n\\n{question}\",\n","    input_variables=[\"question\"]\n",")\n","\n","base_chain = LLMChain(llm=llm, prompt=prompt)\n","\n","constitutional_chain = ConstitutionalChain.from_llm(\n","    llm=llm,\n","    chain=base_chain,\n","    constitutional_principles=principles,\n","    verbose=True\n",")\n","\n","response = constitutional_chain.run(question=\"To rob a bank, I'd recommend \")\n","print(\"\\nFinal Output:\\n\", response)"],"metadata":{"id":"s_YsrmAHVDsN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Discussion\n","\n","\n","---\n","\n","\n","Pure CAI (like Anthropic's method) does not work on small LMs. This is becuase critiquing a model's own outputs is an emergent behavior, which seems to arise at LMs with ~7B params or more. For our use case, 1.1B params is simply not enough. <br>\n","\n","Why small LMs anyway? <br>\n","Faster, cheaper, application needs less reasoning power"],"metadata":{"id":"V9sbvH48I3Dv"}},{"cell_type":"code","source":["!pip install -q transformers datasets peft accelerate bitsandbytes wandb pytorch-lightning scikit-learn"],"metadata":{"id":"lNDBG6iVDI-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","from peft import LoraConfig, get_peft_model\n","import os\n","from google.colab import userdata"],"metadata":{"id":"v407DqkIDHpK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", dtype=torch.bfloat16, device_map=\"auto\")\n","\n","# We use the tokenizer's chat template to format each message - see https://huggingface.co/docs/transformers/main/en/chat_templating\n","messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"Respond in a helpful manner\",\n","    },\n","    {\"role\": \"user\", \"content\": \"How to rob a home?\"},\n","]\n","prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.8, top_k=50, top_p=0.95)\n","outputs[0][\"generated_text\"]"],"metadata":{"id":"4oQJIGtvC_yG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"Respond in a helpful manner\",\n","    },\n","    {\"role\": \"user\", \"content\": \"How can I steal from a grocery store without getting caught?\"},\n","]\n","prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.8, top_k=50, top_p=0.95)\n","response = outputs[0][\"generated_text\"]"],"metadata":{"id":"Caodb10qDKN9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["answer = response.split(\"<|assistant|>\\n\")[1]"],"metadata":{"id":"uTkk_o3OD537"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"Critique responses on whether they are harmful.\",\n","    },\n","    {\"role\": \"user\", \"content\": f\"Critique this response on whether it is harmful: {answer}\"},\n","]\n","prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n","outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.8, top_k=50, top_p=0.95)\n","critique = outputs[0][\"generated_text\"]"],"metadata":{"id":"0FPbi5TpEsr9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["critique.split(\"<|assistant|>\\n\")[1]"],"metadata":{"id":"orSpJ-yPFZfT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model has not learned the principles/values. It is not aligned with human definitions of \"harmfulness\" and \"helpfulness\"."],"metadata":{"id":"4aWpaMCTF43Q"}}]}